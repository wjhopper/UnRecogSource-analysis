---
title: "Source Memory for Unrecognized Items: Descriptive Analysis"
author: "Will Hopper"
date: "`r format(Sys.Date(), '%m-%d-%Y')`"
output:
  html_document:
    fig_retina: 1
    fig_width: 6.5
    fig_height: 6.5
    toc: true
---

```{css, echo=FALSE}
p, li {
  font-size: 16px;
}

img {
  max-width: none;
}

.main-container {
  max-width: 1100px !important;
}
```

```{r include=FALSE}
knitr::opts_chunk$set(echo=FALSE, fig.align = "center")
library(rprojroot)
library(ggplot2)
library(ggExtra)
library(scales)
library(tidyr)
library(dplyr)
root_dir <- rprojroot::is_rstudio_project$find_file()
source(file.path(root_dir, "R", "preprocessing.R"))
```

```{r load_data}
unrecog_source <- load_data(file.path(root_dir, "data"))

# Remove trials with no response from further analysis
missing_trials <- filter(unrecog_source, is.na(response))
percent_missing <- nrow(missing_trials)/nrow(unrecog_source)
unrecog_source <- filter(unrecog_source, !is.na(response))

# Separate null/catch trials from real trials
catch_trials <- filter(unrecog_source, null)
unrecog_source <- filter(unrecog_source, !null)
# Separate source and recognition task trials, for convienience
source_data <- filter(unrecog_source, task == "source")
recog_data <- filter(unrecog_source, task == "recog")
```


## Experimental Design

The experiment was an item recognition experiment (using word stimuli) that also included an item source judgement. Importantly, the source judgement task *preceded* the item recognition decision. This allows assessment of source memory strength without introducing confounds related to thr recognition task (i.e., recollecting the source during the item test, or learning from the recognition test itself).

Several factors were manipulated during the encoding and item recognition phases:

- **Word Frequency**: Words used as targets and lures were either "High Frequency" or "Low Frequency" words.
- **Location/Encoding Task**: The to-be-remembered words were presented in either the top-right or bottom-left corner of the monitor. This location formed the basis for the source-memory judgement. To enhance location memory, participants performed a different encoding task (pleasantness rating, or imageability rating) for items presented at each location. This location:encoding task mapping was held constant over the entire experiment.
- **Decision Risk (bias)**: The risk vs. reward for each recognition response varied from trial to trial. On half the trials, "Old" was a risky response (earning +1 points for a correct "old" response but losing -3 points for an incorrect "old" response), while "New" was a safer response (earning +3 points for a correct "new" response but losing just -1 points for an incorrect "new" response). This payoff scheme should induce a **conservative response bias** in participants (i.e., relatively *un*willing to respond "old"). 

  On the other half of trials, the payoff scheme was reversed: "New" was a risky response (earning +1 points for a correct "new" response but losing -3 points for an incorrect "new" response), while "Old" was a safer response (earning +3 points for a correct "old" response but losing just -1 points for an incorrect "old" response). This payoff scheme should induce a **liberal response bias** in participants (i.e., relatively willing to respond "old").

Together, these manipulations formed a 2 × 2 × 2 factorial design. The participants (N = `r length(unique(catch_trials$subj))`) studied 240 words, organized into 5 lists of 48 words. Each list of studied words was followed by a 30s. distractor task, then the source judgment task, then the item recognition task. In the item recognition task, studied words were mixed with an equal number of unstudied words (lures), for total of 98 recognition judgments for each list, and 480 item recognition judgments in the entire experiment.

20 item recognition trials for each subject were randomly chosen to be "null" or "catch" trials. On these trials, the word did not appear to participants, but they were still required to make an old/new response. These "catch" trials were included to asses how participants used the payoff/bias information. On trials where the item is not shown (thus excluding memory information from entering the decision making process), the normative choice would be to select whatever alternative is the "safe" alternative on this particular trial. See [below](#catch-trial-choice-pattern) for an analysis of these trials.

In all analysis below, catch trials and trials with no response are excluded (except when analyzing catch trials specifically). Overall, only `r sprintf("%.2f%%", percent_missing*100)` of trials were missing due to non-response.

## Overall Accuracy

The scatterplot below shows the relationship between overall item recognition accuracy and overall source memory accuracy for each subject, and a histogram of each variable in the margins.

```{r accuracy}
subject_acc <- group_by(unrecog_source, subj, task) %>%
  summarise(acc = mean(corr))

{ ggplot(spread(subject_acc, task, acc),
       aes(x=source, y=recog)) +
  geom_point(size= 2) +
  scale_x_continuous("Source Memory Accuracy", limits = c(0,1),
                     labels = scales::percent) +
  scale_y_continuous("Recognition Memory Accuracy", limits = c(0,1),
                     labels = scales::percent) +
  coord_fixed() +
  ggtitle("Source vs. Recognition Memory") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5))
} %>%
  ggMarginal(type = "histogram")
```
 Performance is good on both tasks, and performance on the the two tasks appears strongly correlated. There is only one outlier of the relationship, subject `r filter(ungroup(subject_acc), acc==min(acc))[["subj"]]`, who shows relatively poor item recognition accuracy.


## Encoding Task Comparison
The histograms below show the distribution of source and item recognition accuracy across subjects, for both the Pleasantness and Imageability encoding tasks.

```{r encoding_tasks, fig.width=11}
encoding_tasks <- filter(unrecog_source, type != "lure") %>%
  group_by(subj, encoding, task) %>%
  summarise(acc = mean(corr))

ggplot(encoding_tasks, aes(x=acc, fill=encoding)) +
  geom_histogram(bins=20, alpha=.4, position="identity") +
  facet_grid(~task, labeller = as_labeller(c("source"="Source",
                                             "recog"="Recognition"))
             ) +
  scale_fill_discrete("Encoding",
                      labels=c('imageability'='Imageability',
                                'pleasantness'="Pleasantness")) +
  scale_x_continuous("Hit Rate (studied items only)") +
  scale_y_continuous("Number of Subjects") +
  ggtitle("Accuracy by Encoding Task") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5, margin = margin(b=0)),
        legend.position = "top",
        legend.box.margin = margin(t=5,b=5),
        legend.margin = margin(b=-19),
        plot.margin = margin(1,1,2,1))
```

The lack of separation between the histograms in each panel suggests that each encoding task was equally effective. In other words, it appears there is no difference in source or item accuracy between items studied using a pleasantness rating task and items studied using an imageability rating task.

## Word Frequency Effects
The scatterplots below show the relationship of source and item recognition accuracy across subjects, separately for high and low frequency words.

```{r word_frequency, fig.width=11}
word_freq <- group_by(unrecog_source, subj, wf, task) %>%
  summarise(acc = mean(corr))

ggplot(spread(word_freq, task, acc), aes(x=source, y=recog)) +
  geom_point(size=2) +
  geom_rug(size=0.1) +
  facet_grid(~wf, labeller = as_labeller(c("LF"="Low Frequency",
                                           "HF"="High Frequency"))
             ) +
  scale_x_continuous("Source Memory Accuracy", limits = c(0,1),
                     labels = scales::percent) +
  scale_y_continuous("Recognition Memory Accuracy", limits = c(0,1),
                     labels = scales::percent) +
  coord_fixed() +
  ggtitle("Accuracy by Word Frequency") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5))
```

The linear relationship between source and item recognition accuracy seems to be similar for both the high and low frequency word groups. There appears to be a small effect of word frequency on item recognition accuracy, with sliglty higher recognition accuracy for low frequency words. There does not appear to be an effect of word frequency on source accuracy.

## Bias Effects

### ROC
The first scatterplot below shows the observed Hit and False Alarm rate pairs for each subject in each bias condition. The lines between points connect observations from the same subject. The group average in each bias condition is shown with the larger "X" points.

```{r bias}
bias_effects <- group_by(recog_data, subj, bias, type) %>%
  summarise(p = mean(corr)) %>%
  spread(type, p) %>%
  mutate(lure = 1-lure,
         diff = target-lure) %>%
  rename(HR = target, FAR = lure)

avg_bias_effects <- group_by(bias_effects, bias) %>%
  summarise_at(c("FAR", "HR"), mean)

ggplot(bias_effects, aes(x=FAR, y=HR, color=bias)) +
  geom_point(alpha=.5) +
  geom_path(mapping=aes(group=subj), color="black") +
  geom_point(data=avg_bias_effects, stroke=3, size=3, shape=4) +
  scale_color_discrete("Bias", labels=c("liberal"="Liberal",
                                        "conservative"="Conservative")
                       ) +
  ggtitle("Bias Manipulation Effects") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5, margin = margin(b=-1)),
        legend.position = "top",
        legend.box.margin = margin(t=2,b=2),
        legend.margin = margin(b=-15),
        plot.margin = margin(1,1,2,1))
```

A shift from a conservative response bias to a liberal response bias should be associated with an increase in both the hit and false alarm rates. This pattern is observed in the aggregated data, but is not seen for every subject. Several subjects (e.g., subjects 1, 103, 104, and 105) show the opposite pattern - a *decrease* in hits and false alarms in the "conservative" condition.

### Catch Trial Choice Pattern
To further examine participants sensitivity to the payoff/bias manipulation, their pattern of responding on the "catch" trials was examined. The scatterplot below shows the number of "old" and "new" responses made on these catch trials in each bias condition, with "old" along the Y axis and "new" along the X axis. The major diagonal line (i.e., equal "old" and "new" responses) is included for referece.

```{r catch_trials}
catch_effects <- filter(catch_trials, task=="recog") %>%
  count(subj, bias, response) %>%
  complete(subj, bias, response, fill = list(n=0)) %>% # not 100% sure about this step...
  spread(response, n) %>%
  rename(new=m, old=z)

ggplot(catch_effects, aes(x=old, y=new, color=bias)) +
  geom_point(size=2) +
  geom_abline(slope=1,intercept=0) +
  scale_color_discrete("Bias", labels=c("liberal"="Liberal",
                                      "conservative"="Conservative")
                     ) +
  coord_fixed() +
  ggtitle("Catch trial decision pattern") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5, margin = margin(b=-1)),
        legend.position = "top",
        legend.box.margin = margin(t=2,b=2),
        legend.margin = margin(b=-15),
        plot.margin = margin(1,1,2,1))
```

If participants were sensitive to the payoff manipulation, then they would respond "old" more frequently when it is the safer response (i.e., the liberal condition), and would respond "new" more frequently when it is the safer response (i.e., the conservative condition). This pattern would manifest visually as a separation of the conservative and liberal conditions above and below the major diagonal, respectively.

This pattern appears to roughly hold, though it the separation is less distict for reponses made under the "liberal" bias payoff. On the whole, the pattern of catch trial responses indicates participants were at least *able* to be sensitive to the payoff schemes.

## Conditional Source Accuracy

The main thrust of the experiment was to examine source accuracy for studied but laster unrecognized items. The dot plot below shows source accuracy for studied items in each bias condition, conditional on whether the item was recognized ("hits") or unrecognized ("misses") on the item recognition test. The small, transparent dots represent individual subjects, while the larger "X"'s represent the group average.

```{r conditional}
conditional_acc_by_subj <- 
  inner_join(select(source_data, subj, item, bias, source_acc = corr),
             select(recog_data, subj, item, bias, recog_acc = corr),
             by = c("subj","item","bias")) %>%
  group_by(subj, bias, recog_acc) %>%
  summarise(source_acc = mean(source_acc),
            n_obs = n())

conditional_acc <- group_by(conditional_acc_by_subj, bias, recog_acc) %>%
  summarise(source_acc = mean(source_acc))

effect <- filter(conditional_acc, bias=="conservative", recog_acc==0)$source_acc - 
  filter(conditional_acc, bias=="liberal", recog_acc==0)$source_acc
```

```{r conditional_plots}
ggplot(conditional_acc_by_subj, aes(x=factor(recog_acc), y=source_acc, color=bias)) +
  geom_point(position = position_dodge(width=.5), alpha=.5, shape=16, size=2) +
  geom_point(data=conditional_acc, position = position_dodge(width=.5),
             size=4, shape=4, stroke=3) +
  scale_x_discrete("Studied Item Response Type", labels=c(`0`="Miss", `1`="Hit")) +
  scale_y_continuous("Source Accuracy") +
  scale_color_discrete("Bias") +
  ggtitle("Conditional Source Accuracy") +
  theme_bw(base_size = 16) +
  theme(plot.title = element_text(hjust=0.5, margin = margin(b=-1)),
        legend.position = "top",
        legend.box.margin = margin(t=2,b=2),
        legend.margin = margin(b=-15),
        plot.margin = margin(1,1,2,1))
```

Overall, source accuracy is moderately higher (about `r sprintf("%.2f%%", effect*100)`) for items "missed" under a conservative threshold than items "missed" under a liberal threshold.
